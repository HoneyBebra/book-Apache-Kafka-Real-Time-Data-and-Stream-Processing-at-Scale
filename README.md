### Открываем для себя систему kafka

Сообщения в кафке хранятся в топиках (ближ. аналог - таблица), топики в свою очередь разделены на партиции (даёт возможность горизонтального масштабирования, если, например, разделять партиции по серверам), данные в партициях отсортированы, но это не гарантирует отсортированность данных во всём топике.
Помимо этого есть реплики разделов для отказоустойчивости.

#### Продюсеры и консюмеры
Кафка устроена достаточно легко, продюсеры сами складывают сообщения в топик, консюмеры сами читают эти сообщения. Кафка только запоминает оффсет - индекс, на котором остановился читать консюмер (или группа консюмеров), чтобы потом можно было продолжить с прежнего места. По умолчанию продюсер кидает данные в разные партиции равномерно, но и конкретную можно задать.

#### Кластер kafka
![cluster_schema](img/cluster_schema.png)

Какой-то из живых брокеров автоматически выбирается ведущим и выполняет административную работу (распледеление партиций, мониторинг отказов брокеров), так же один брокер может содержать реплику партиции другого брокера, чтобы обеспечить отказоустойчивость.

### Интересные настройки

* num.recovery.threads.per.data.dir - потоки, используемые для обработки сегментов журналов, используются только при запуске и остановке кластера, поэтому можно задать число побольше (по дефолту 1)
* auto.leader.rebalance.enable - чтобы балансировать ведущие реплики на разных брокерах (чтобы все они не были на одном). Происходит это раз в какое-то время (параметр *leader.imbalance.interval.seconds*) или если превышен дисбаланс (параметр *leader.imbalance.per.broker.percentage*)
* default.replication.factor - рекоммендованное значение 2 или 3, если имеется достаточно мощности, так будет обеспечиваться одно запланированное, и одно незапланированное отключение брокеров с сохранением доступности всех данных
* log.retention.bytes - ограничение вместимости топика по объёму памяти
* log.retention.ms - топик чистится раз в какое-то время
* log.segment.bytes - 
